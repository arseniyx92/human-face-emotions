# @package _global_
training:
  max_epochs: 2
  batch_size: 512
  lr: 0.003
  weight_decay: 0.01
  optimizer: adamw
  scheduler:
    name: reduce_on_plateau
    factor: 0.5
    patience: 10
  early_stopping:
    patience: 20
    monitor: val_loss
    mode: min
  accelerator: gpu
  devices: 1
  num_workers: 13
  checkpoint:
    monitor: val_loss
    save_top_k: 3
    mode: min
